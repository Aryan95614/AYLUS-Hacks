{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObS4UWfKHBsytcrro2BWmO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan95614/AYLUS-Hacks/blob/RUNTHISFILE/Pytorch_Tutorial_1_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wKQ8SMDMAxms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4d5e66-2c23-4ab8-f85b-4c4d3caebbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# Making sure that Pytorch is here Installed\n",
        "import torch \n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are the fundamental building block of machine learning.\n",
        "\n",
        "Their job is to represent data in a numerical way.\n",
        "\n",
        "For example, you could represent an image as a tensor with shape [3, 224, 224] which would mean [colour_channels, height, width], as in the image has 3 colour channels (red, green, blue), a height of 224 pixels and a width of 224 pixels.\n",
        "\n",
        "You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side.\n"
      ],
      "metadata": {
        "id": "4YaOaWCIFh-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Scalar -> Zero Dimension Tensor\n",
        "Scaler = torch.tensor(7)\n",
        "print(Scaler.ndim) # 0 because it shows the dimensions as we said\n",
        "print(Scaler.item())"
      ],
      "metadata": {
        "id": "Gq7kcqxPFjPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vector is a single dimension tensor but can contain many numbers.\n",
        "\n",
        "As in, you could have a vector [3, 2] to describe [bedrooms, bathrooms] in your house. Or you could have [3, 2, 2] to describe [bedrooms, bathrooms, car_parks] in your house.\n",
        "\n",
        "The important trend here is that a vector is flexible in what it can represent (the same with tensors)."
      ],
      "metadata": {
        "id": "vAxh0L7AG5Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Vector -> Flexible it what it can be\n",
        "Vector = torch.tensor([7, 7])\n",
        "print(Vector.shape) #torch.Size([2]): tells you how it is shaped like that"
      ],
      "metadata": {
        "id": "B4pB-UH9G6DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matrix is a bit different but will have one more dimension compared to a vector, it also has the same characterisitics.\n"
      ],
      "metadata": {
        "id": "lp-3JC_ZIvlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix\n",
        "MATRIX = torch.tensor([[7, 8], \n",
        "                       [9, 10]])\n",
        "print(MATRIX.ndim)  # 2: We can tell that this already 2 dimensional and unlike a vector can carry more paramaters\n",
        "print(MATRIX.shape) # torch.Size([2, 2]): We can see two subarrays with two elements, making it 2-2"
      ],
      "metadata": {
        "id": "4Bl7Y9wUI3lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor\n",
        "Tensor = torch.tensor([[[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]]])\n",
        "print(Tensor.ndim) # 3 This is 3 dimensional \n",
        "print(Tensor.shape)# torch.Size([1, 3, 3]) Do it in reverse and you will be able to tell"
      ],
      "metadata": {
        "id": "SM5Vd7j-ROrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've established tensors represent some form of data.\n",
        "\n",
        "And machine learning models such as neural networks manipulate and seek patterns within tensors.\n",
        "\n",
        "But when building machine learning models with PyTorch, it's rare you'll create tenors by hand (like what we've being doing).\n",
        "\n",
        "Instead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.\n",
        "\n",
        "In essence:\n",
        "\n",
        "Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers...\n",
        "\n",
        "As a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers."
      ],
      "metadata": {
        "id": "j55iwFMtp0Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can generate random tensors that already have their data fitted into them \n",
        "\n",
        "# Create a random tensor of size (3, 4)\n",
        "random_tensor = torch.rand(size=(3, 4))\n",
        "print(random_tensor)\n",
        "print(random_tensor.dtype)\n",
        "\n",
        "# Making sure that only 0s and 1s are filled in it\n",
        "zeros = torch.zeros(size=(3, 4))\n",
        "print(zeros)\n",
        "print(zeros.dtype)\n",
        "\n",
        "ones = torch.ones(size=(3, 4))\n",
        "print(ones)\n",
        "print(ones.dtype , \"\\n\")\n",
        "\n",
        "# Creating any sort of ranges and creating a range of values 0 to 10\n",
        "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
        "print(zero_to_ten)\n",
        "\n",
        "# Creating a comparitive tensor of zeros\n",
        "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
        "print(ten_zeros)"
      ],
      "metadata": {
        "id": "TXpgBIyHp1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tensor datatypes\n",
        "There are many different tensor datatypes available in PyTorch.\n",
        "[link](https://pytorch.org/docs/stable/tensors.html#data-types) \n",
        "Some are specific for CPU and some are better for GPU.\n",
        "\n",
        "Getting to know which is which can take some time.\n",
        "\n",
        "Generally if you see torch.cuda anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).\n",
        "\n",
        "The most common type (and generally the default) is torch.float32 or torch.float.\n",
        "\n",
        "This is referred to as \"32-bit floating point\".\n",
        "\n",
        "But there's also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\n"
      ],
      "metadata": {
        "id": "p_cdUSmlmowQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default datatype for tensors is float32\n",
        "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
        "                               device=None, # defaults to None, which uses the default tensor type\n",
        "                               requires_grad=False) # if True, operations perfromed on the tensor are recorded \n",
        "\n",
        "print(float_32_tensor.shape)\n",
        "print(float_32_tensor.dtype)\n",
        "print(float_32_tensor.device) # cpu written(PyTorch likes calculations between tensors to be on the same device).\n",
        "\n",
        "\n",
        "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=torch.float16) # torch.half would also work\n",
        "print(float_16_tensor.dtype)"
      ],
      "metadata": {
        "id": "8MAeoW5xmpJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting information from tensors\n",
        "\n",
        "Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.\n",
        "\n",
        "We've seen these before but three of the most common attributes you'll want to find out about tensors are:\n",
        "\n",
        "shape - what shape is the tensor? (some operations require specific shape rules)\n",
        "dtype - what datatype are the elements within the tensor stored in?\n",
        "device - what device is the tensor stored on? (usually GPU or CPU)\n",
        "Let's create a random tensor and find out details about it."
      ],
      "metadata": {
        "id": "24ssEaPEoGkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "some_tensor = torch.rand(3, 4)\n",
        "\n",
        "# Find out details about it\n",
        "print(some_tensor)\n",
        "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n",
        "\n",
        "\n",
        "#Note: When you run into issues in PyTorch, it's very often\n",
        "#one to do with one of the three attributes above. \n",
        "#So when the error messages show up, sing yourself a \n",
        "#little song called \"what, what, where\":\n",
        "\n",
        "#\"what shape are my tensors? what datatype are they and where are they stored? \n",
        "#what shape, what datatype, where where where\"\n",
        "\n"
      ],
      "metadata": {
        "id": "H1AifcXKoKr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manipulating tensors (tensor operations)\n",
        "In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\n",
        "\n",
        "A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\n",
        "\n",
        "These operations are often a wonderful dance between:\n",
        "\n",
        "- Addition\n",
        "- Substraction\n",
        "- Multiplication (element-wise)\n",
        "- Division\n",
        "- Matrix multiplication\n",
        "And that's it. Sure there are a few more here and there but these are the basic building blocks of neural networks.\n",
        "\n",
        "Stacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!)."
      ],
      "metadata": {
        "id": "M3pe5o2UoXyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor of values and add a number to it\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "tensor + 10"
      ],
      "metadata": {
        "id": "gI4ghDWJpE74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}